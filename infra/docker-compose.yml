services:
  gateway:
    build: .
    ports:
      - "8000:8000"
    environment:
      - DEBUG=${DEBUG:-false}
      - LOG_LEVEL=${LOG_LEVEL:-INFO}
      - REDIS_HOST=redis
      - REDIS_PORT=6379
      - REDIS_PASSWORD=${REDIS_PASSWORD:?REDIS_PASSWORD is required}
    depends_on:
      redis:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "python", "-c", "import httpx; r = httpx.get('http://localhost:8000/health'); r.raise_for_status()"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s

  redis:
    image: redis:7-alpine
    command: redis-server /etc/redis/redis.conf --requirepass ${REDIS_PASSWORD:?REDIS_PASSWORD is required}
    environment:
      - REDIS_PASSWORD=${REDIS_PASSWORD}
      - REDISCLI_AUTH=${REDIS_PASSWORD}
    volumes:
      - redis_data:/data
      - ./redis.conf:/etc/redis/redis.conf:ro
    healthcheck:
      test: ["CMD-SHELL", "redis-cli ping | grep -q PONG"]
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 5s

  prometheus:
    image: prom/prometheus:v2.53.0
    ports:
      - "9091:9090"
    volumes:
      - ./prometheus.yml:/etc/prometheus/prometheus.yml
      - prometheus_data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'

  grafana:
    image: grafana/grafana:10.2.4
    ports:
      - "3000:3000"
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=${GRAFANA_ADMIN_PASSWORD:?GRAFANA_ADMIN_PASSWORD is required}
    volumes:
      - grafana_data:/var/lib/grafana

  # vLLM Inference Service
  # Choose ONE configuration below by uncommenting the desired option
  # Note: Health monitoring is handled by the gateway, not Docker health checks
  
  # Option 1: GPU-accelerated vLLM (recommended for production)
  # Uncomment this section for NVIDIA GPU acceleration
  # vllm:
  #   build: ../services/vllm-inference
  #   ports:
  #     - "${VLLM_PORT:-8080}:${VLLM_PORT:-8080}"
  #   environment:
  #     - VLLM_MODEL=${VLLM_MODEL:-google/gemma-3-270m}
  #     - VLLM_PORT=${VLLM_PORT:-8080}
  #     - VLLM_HOST=${VLLM_HOST:-0.0.0.0}
  #     - VLLM_MAX_MODEL_LEN=${VLLM_MAX_MODEL_LEN:-2048}
  #     - VLLM_GPU_MEMORY_UTILIZATION=${VLLM_GPU_MEMORY_UTILIZATION:-0.7}
  #     - VLLM_TENSOR_PARALLEL_SIZE=${VLLM_TENSOR_PARALLEL_SIZE:-1}
  #     - VLLM_TRUST_REMOTE_CODE=${VLLM_TRUST_REMOTE_CODE:-false}
  #     - VLLM_ENABLE_GPU=true
  #     - HUGGING_FACE_HUB_TOKEN=${HUGGING_FACE_HUB_TOKEN}
  #   volumes:
  #     - vllm_cache:/root/.cache/huggingface
  #   deploy:
  #     resources:
  #       limits:
  #         cpus: '2.0'
  #         memory: 3G
  #       reservations:
  #         devices:
  #           - driver: nvidia
  #             count: 1
  #             capabilities: [gpu]

  # Option 2: CPU-only vLLM (for systems without GPU)
  # Uncomment this section for CPU-only inference
  vllm:
    build: ../services/vllm-inference
    ports:
      - "${VLLM_PORT:-8080}:${VLLM_PORT:-8080}"
    environment:
      - VLLM_MODEL=${VLLM_MODEL:-google/gemma-3-270m}
      - VLLM_PORT=${VLLM_PORT:-8080}
      - VLLM_HOST=${VLLM_HOST:-0.0.0.0}
      - VLLM_MAX_MODEL_LEN=${VLLM_MAX_MODEL_LEN:-1024}
      - VLLM_TENSOR_PARALLEL_SIZE=1
      - VLLM_TRUST_REMOTE_CODE=${VLLM_TRUST_REMOTE_CODE:-false}
      - VLLM_ENABLE_GPU=${VLLM_ENABLE_GPU:-false}
      - HUGGING_FACE_HUB_TOKEN=${HUGGING_FACE_HUB_TOKEN}
    volumes:
      - vllm_cache:/root/.cache/huggingface
    deploy:
      resources:
        limits:
          cpus: '2.0'
          memory: 2G
        reservations:
          cpus: '1.0'
          memory: 1G

volumes:
  redis_data:
  prometheus_data:
  grafana_data:
  vllm_cache:
