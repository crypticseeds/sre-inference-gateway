# Production overrides
# Use: docker-compose -f docker-compose.yml -f docker-compose.prod.yml up

services:
  gateway:
    environment:
      - DEBUG=false
      - LOG_LEVEL=WARN
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: '2.0'
          memory: 2G

  redis:
    restart: unless-stopped
    volumes:
      - redis_data:/data
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 512M

  prometheus:
    restart: unless-stopped
    volumes:
      - prometheus_data:/prometheus
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 1G

  grafana:
    restart: unless-stopped
    environment:
      - GF_SECURITY_ADMIN_USER=${GRAFANA_ADMIN_USER:-admin}
      - GF_SECURITY_ADMIN_PASSWORD=${GRAFANA_ADMIN_PASSWORD:?GRAFANA_ADMIN_PASSWORD is required}
    volumes:
      - grafana_data:/var/lib/grafana
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 512M

  # vLLM Inference Service - Production Configuration
  # Choose ONE configuration below by uncommenting the desired option
  
  # Option 1: GPU-accelerated vLLM (recommended for production workloads)
  # Uncomment this section for NVIDIA GPU acceleration
  # vllm:
  #   restart: unless-stopped
  #   environment:
  #     - VLLM_MODEL=${VLLM_MODEL:-google/gemma-3-270m}
  #     - VLLM_PORT=${VLLM_PORT:-8080}
  #     - VLLM_HOST=${VLLM_HOST:-0.0.0.0}
  #     - VLLM_MAX_MODEL_LEN=${VLLM_MAX_MODEL_LEN:-2048}
  #     - VLLM_GPU_MEMORY_UTILIZATION=${VLLM_GPU_MEMORY_UTILIZATION:-0.7}
  #     - VLLM_TENSOR_PARALLEL_SIZE=${VLLM_TENSOR_PARALLEL_SIZE:-1}
  #     - VLLM_TRUST_REMOTE_CODE=${VLLM_TRUST_REMOTE_CODE:-false}
  #     - VLLM_ENABLE_GPU=true
  #     - HUGGING_FACE_HUB_TOKEN=${HUGGING_FACE_HUB_TOKEN}
  #   deploy:
  #     resources:
  #       limits:
  #         cpus: '3.0'
  #         memory: 4G
  #       reservations:
  #         cpus: '2.0'
  #         memory: 2G
  #         devices:
  #           - driver: nvidia
  #             count: 1
  #             capabilities: [gpu]

  # Option 2: CPU-only vLLM (for production systems without GPU)
  # Uncomment this section for CPU-only inference
  vllm:
    restart: unless-stopped
    environment:
      - VLLM_MODEL=${VLLM_MODEL:-google/gemma-3-270m}
      - VLLM_PORT=${VLLM_PORT:-8080}
      - VLLM_HOST=${VLLM_HOST:-0.0.0.0}
      - VLLM_MAX_MODEL_LEN=${VLLM_MAX_MODEL_LEN:-1024}
      - VLLM_TENSOR_PARALLEL_SIZE=1
      - VLLM_TRUST_REMOTE_CODE=${VLLM_TRUST_REMOTE_CODE:-false}
      - VLLM_ENABLE_GPU=false
      - HUGGING_FACE_HUB_TOKEN=${HUGGING_FACE_HUB_TOKEN}
    deploy:
      resources:
        limits:
          cpus: '1.0'
          memory: 1G
        reservations:
          cpus: '1.0'
          memory: 1G

volumes:
  redis_data:
  prometheus_data:
  grafana_data:
  vllm_cache:
