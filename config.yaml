# SRE Inference Gateway Configuration
# This file contains the main configuration for the gateway service

version: "0.1.0"

# Server configuration
server:
  host: "0.0.0.0"
  port: 8000
  debug: false

# Provider configurations
providers:
  - name: "openai"
    type: "openai"
    weight: 0.5
    enabled: false  # Disabled by default, enable when API key is configured
    base_url: "https://api.openai.com/v1"
    api_key_env: "OPENAI_API_KEY"  # Environment variable for API key
    health_check_url: null
    timeout: 30.0
    max_retries: 3
  
  - name: "vllm"
    type: "vllm"
    weight: 0.5
    enabled: false  # Disabled by default, enable when vLLM service is running
    base_url: "http://localhost:8080/v1"  # Changed from 8000 to avoid gateway port conflict
    api_key_env: null  # vLLM doesn't require API key
    health_check_url: null
    timeout: 30.0
    max_retries: 3
  
  - name: "mock_openai"
    type: "mock"
    weight: 0.5
    enabled: true
    health_check_url: null
    timeout: 30.0
    max_retries: 3
  
  - name: "mock_vllm"
    type: "mock"
    weight: 0.5
    enabled: true
    health_check_url: null
    timeout: 30.0
    max_retries: 3

# Health check configuration
health:
  check_interval: 30.0  # Health check interval in seconds
  timeout: 5.0          # Health check timeout in seconds
  retries: 3            # Number of retries for failed health checks

# Logging configuration
logging:
  level: "INFO"
  format: "%(asctime)s - %(name)s - %(levelname)s - %(message)s"

# Metrics configuration
metrics:
  enabled: true
  port: 9090

# Request processing limits
max_request_size: 1048576  # 1MB in bytes
request_timeout: 35.0      # Request timeout in seconds (slightly larger than provider timeout to avoid races)